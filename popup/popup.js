// popup.js
console.log("âœ… VoiceNav popup.js loaded and ready.");

// Get UI elements
const startBtn = document.getElementById("startBtn");
const stopBtn = document.getElementById("stopBtn");
const statusEl = document.getElementById("status");
const commandText = document.getElementById("commandText");

let recognition;

// Logging helper
function log(message) {
  console.log(message);
}

// ğŸ§  Function: Start listening for voice input
async function startListening() {
  try {
    log("ğŸ™ Start Listening clicked.");
    statusEl.textContent = "Status: Requesting microphone access...";
    commandText.textContent = "";

    // Check if SpeechRecognition API is supported
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) {
      alert("Speech recognition not supported in this browser.");
      return;
    }

    // Ask for mic permission first (avoids silent denials)
    log("ğŸ¤ Requesting microphone permission...");
    try {
      await navigator.mediaDevices.getUserMedia({ audio: true });
      log("âœ… Microphone permission granted.");
    } catch (err) {
      log("ğŸš« Microphone permission blocked: " + err.message);
      alert(
        "VoiceNav needs microphone access.\n\nPlease click the microphone icon in Chromeâ€™s address bar and choose 'Allow'."
      );
      statusEl.textContent = "Status: Mic permission denied.";
      return;
    }

    // Initialize Speech Recognition
    recognition = new SpeechRecognition();
    recognition.lang = "en-US";
    recognition.continuous = false;
    recognition.interimResults = false;

    // When recognition starts
    recognition.onstart = () => {
      statusEl.textContent = "Status: Listening...";
      startBtn.disabled = true;
      stopBtn.disabled = false;
      log("ğŸ§ Listening...");
    };

    // When speech is recognized
    recognition.onresult = async (event) => {
      const text = event.results[0][0].transcript;
      commandText.textContent = text;
      statusEl.textContent = "Status: Processing...";
      log("ğŸ—£ Recognized text:", text);

      // Send the command to the active tab (content.js)
      const [tab] = await chrome.tabs.query({ active: true, currentWindow: true });
      if (tab) {
        chrome.tabs.sendMessage(tab.id, { type: "VOICE_COMMAND", text });
        log("ğŸ“¤ Command sent to content.js:", text);
      } else {
        log("âš ï¸ No active tab found.");
      }

      statusEl.textContent = "Status: Idle";
    };

    // Error handler
    recognition.onerror = (e) => {
      console.error("âŒ Recognition error:", e.error);
      statusEl.textContent = "Error: " + e.error;

      if (e.error === "not-allowed") {
        log("ğŸ”’ Microphone permission denied. Check Chrome settings â†’ Site settings â†’ Microphone.");
      }
    };

    // When recognition stops
    recognition.onend = () => {
      log("ğŸ”š Recognition ended.");
      startBtn.disabled = false;
      stopBtn.disabled = true;
      statusEl.textContent = "Status: Idle";
    };

    recognition.start();
  } catch (err) {
    console.error("âŒ Unexpected error in startListening():", err);
    statusEl.textContent = "Error: " + err.message;
  }
}

// ğŸ›‘ Stop listening
function stopListening() {
  if (recognition) {
    recognition.stop();
    statusEl.textContent = "Status: Stopped.";
    startBtn.disabled = false;
    stopBtn.disabled = true;
    log("ğŸ›‘ Listening stopped by user.");
  }
}

// ğŸ§© Event listeners
startBtn.addEventListener("click", startListening);
stopBtn.addEventListener("click", stopListening);
